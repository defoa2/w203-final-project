---
title: "Chicago Public Schools Mathematics and Literacy Skills"
author: 'Austen Lowitz, Annie DeForge, William Teng'
output:
  pdf_document
header-includes:
 \usepackage{wrapfig}
 \usepackage{float}
 \floatplacement{figure}{H}
---

```{r load packages and set options, include=FALSE}
library(tidyverse)
library(magrittr)
library(stargazer)
library(sandwich)
library(lmtest)
library(dplyr)
library(ggplot2)
library(GGally)
library(reshape2)

theme_set(theme_bw())
```

```{r load data, echo=FALSE, message = FALSE, include=FALSE}

# Set columns to focus on
school_columns <- c("Growth.Overall.Reading.and.Math",
                    "Student.Attendance.2012...Percent",
                    "Misconducts.Resulting.in.Suspensions.2012...Percent",
                    "Average.Days.of.Suspension.2012",  
                    "Teacher.Attendance.2012...Percent",
                    "Involved.Families",               
                    "Supportive.Environment",            
                    "Safety",                           
                    "Effective.Leaders",                
                    "Ambitious.Instruction",            
                    "Collaborative.Teachers") 

# Read in initial data : 460 Records
my_data <- read.csv("chicago-public-schools-elementary-school-progress-report-card-2012-2013-1.csv")[, school_columns] 

# Filter out NULLs : 416 Records
my_data <- my_data[complete.cases(my_data[, school_columns]), ]

# Filter out "Not Enough Data" : 282 Records
my_data <- subset(my_data, Involved.Families != "Not Enough Data"
                         & Supportive.Environment != "Not Enough Data"
                         & Safety != "Not Enough Data"
                         & Effective.Leaders != "Not Enough Data"
                         & Ambitious.Instruction != "Not Enough Data"
                         & Collaborative.Teachers != "Not Enough Data")

# Convert Likert variables into ascending scales (1 = Lowest, N = Highest)
my_data$Involved.Families <- factor(my_data$Involved.Families, levels = c("Very Weak", "Weak", "Neutral", "Strong", "Very Strong"), labels = c(1, 2, 3, 4, 5))

my_data$Supportive.Environment <- factor(my_data$Supportive.Environment, levels = c("Very Weak", "Weak", "Neutral", "Strong", "Very Strong"), labels = c(1, 2, 3, 4, 5))

my_data$Safety <- factor(my_data$Safety, levels = c("Very Weak", "Weak", "Neutral", "Strong", "Very Strong"), labels = c(1, 2, 3, 4, 5))

my_data$Effective.Leaders <- factor(my_data$Effective.Leaders, levels = c("Very Weak", "Weak", "Neutral", "Strong", "Very Strong"), labels = c(1, 2, 3, 4, 5))

my_data$Ambitious.Instruction <- factor(my_data$Ambitious.Instruction, levels = c("Very Weak", "Weak", "Neutral", "Strong", "Very Strong"), labels = c(1, 2, 3, 4, 5))

my_data$Collaborative.Teachers <- factor(my_data$Collaborative.Teachers, levels = c("Very Weak", "Weak", "Neutral", "Strong", "Very Strong"), labels = c(1, 2, 3, 4, 5))

# Convert numeric columns into numeric
my_data$Average.Days.of.Suspension.2012 <- as.numeric(my_data$Average.Days.of.Suspension.2012)  

my_data$Involved.Families <- as.numeric(my_data$Involved.Families)

my_data$Supportive.Environment <- as.numeric(my_data$Supportive.Environment)

my_data$Safety <- as.numeric(my_data$Safety)

my_data$Effective.Leaders <- as.numeric(my_data$Effective.Leaders)

my_data$Ambitious.Instruction <- as.numeric(my_data$Ambitious.Instruction)

my_data$Collaborative.Teachers <- as.numeric(my_data$Collaborative.Teachers)

my_data$Growth.Overall.Reading.and.Math <- as.numeric(gsub("%", "", my_data$Growth.Overall.Reading.and.Math)) / 100

my_data$Student.Attendance.2012...Percent <- as.numeric(gsub("%", "", my_data$Student.Attendance.2012...Percent)) / 100

my_data$Teacher.Attendance.2012...Percent <- as.numeric(gsub("%", "", my_data$Teacher.Attendance.2012...Percent)) / 100
                    
my_data$Misconducts.Resulting.in.Suspensions.2012...Percent <- as.numeric(gsub("%", "", my_data$Misconducts.Resulting.in.Suspensions.2012...Percent)) / 100

# Drop nulls in dependent variable. For some reason the code above did not catch all nulls (don't know why)
my_data <- my_data[complete.cases(my_data$Growth.Overall.Reading.and.Math), ]

```

# Introduction

In an effort to enhance education in Chicago, we aim to gain an understanding of the factors that contribute to a positive learning experience for students. We seek to identify evidence-based actionable insights for schools and policymakers throughout the Chicago Public School District to make informed decisions so that students can thrive academically. This can range from staffing and leadership decisions, school security code changes, teaching staff rostering, and discipline policies.

# Data

The 2012 School Progress Report Card data from the City of Chicago data portal provides us with a view of the education landscape. It includes the academic performance growth metric ($Growth.Overall.Reading.and.Math$), school culture insights ($Misconducts.Resulting.in.Suspensions.2012...Percent$,$Average.Days.of.Suspension.2012$ $Student.Attendance.2012...Percent$, $Teacher.Attendance.2012...Percent$), and student support data ($Involved.Families$, $Supportive.Environment$,$Safety$,$Effective.Leaders$, $Ambitious.Instruction$,$Collaborative.Teachers$).

# How Key Concepts are Operationalized

# EDA

```{r Correlation Matrix}
# Install the required packages if not already installed
if (!requireNamespace("reshape2", quietly = TRUE)) {
  install.packages("reshape2")
}

# Compute the correlation between each predictor variable and the dependent variable
correlations <- cor(my_data[, setdiff(names(my_data), "Growth.Overall.Reading.and.Math")], my_data$Growth.Overall.Reading.and.Math)

# Convert the corr matrix to a df for viz
cor_df <- melt(correlations)

# Reorder the levels of the Var1 variable based on the correlation with the dependent variable
cor_df$Var1 <- reorder(cor_df$Var1, cor_df$value, FUN = function(x) -abs(x))

# Create a corr heatmap
heatmap_plot <- ggplot(cor_df, aes(x = Var1, y = Var2, fill = value, label = round(value, 2))) +
  geom_tile() +
  geom_text(color = "black", size = 6, vjust = 1) +  # Add correlation value as text
  scale_fill_gradient(low = "yellow", high = "purple") +
  labs(title = "Correlation of Independent Vars w/ Respect to Dependent Var") +
  labs(x = "", y = "") +  # Remove axis labels for Var1 and Var2
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  coord_fixed()

heatmap_plot <- heatmap_plot + labs(y = "Growth.Overall.Reading.and.Math")

heatmap_plot
```

```{r Create some models}
# Naive Model: A few input variables
model_naive <- lm(Growth.Overall.Reading.and.Math ~ Student.Attendance.2012...Percent +  Involved.Families + Collaborative.Teachers, data = my_data)

# Complex Model: All the variables
model_complex <- lm(Growth.Overall.Reading.and.Math ~  Student.Attendance.2012...Percent + Misconducts.Resulting.in.Suspensions.2012...Percent + Average.Days.of.Suspension.2012 + Teacher.Attendance.2012...Percent + Involved.Families + Supportive.Environment + Safety + Effective.Leaders + Ambitious.Instruction + Collaborative.Teachers, data = my_data)

```

```{r}
summary(model_naive)
```

```{r}
summary(model_complex)
```

# Assumptions

Given our large sample size (218 observations), we can utilize a multiple regression model to assess the key drivers on student success, assuming that our data meets the following criteria: The data must be Independent and Identically Distributed (IID), there must be no perfect collinearity, and appropriate transformations need to be executed (if necessary) to ensure we meet the linear conditional expectation assumption.

[**IID**]{.underline}

Each row in our dataframe represents a different school in Chicago. The researchers who compiled this dataset randomly sampled schools from around the Chicago area, ensuring that each observation is independent from one another. Additionally, we intentionally excluded variables such as zip code and location coordinates, which can lead to clustering and violate the IID assumption. Because of the random sampling techniques and exclusion of certain variables that could introduce bias and clustering, we can safely conclude that our data is IID.

[**No Perfect Collinearity**]{.underline}

Another key assumption in using large linear models is that the predictor variables must have no perfect collinearity. As shown in the correlation matrix below, the strongest correlation between independent variables is between $Collaborative.Teachers$ and $Effective.Leaders$, with a Pearson's Correlation Coefficient of r = 0.78. Because this relationship is still far away from 1, even the strongest relationship between independent variables does not result in near perfect collinearity.

```{r Correlation Matrix}
ggpairs(my_data, columns = school_columns, progress=FALSE)
```

Additionally, we can run Variance Inflation Factor (VIF) on our complex model to better understand how much the variance of the various beta coefficients increases due to multicollinearity. A VIF of less than 5 is indicative of no perfect collineaity. As we can see from the VIF results above, none of our independent variabels have a VIF value of \>= 5. Between the correlation matrix and VIF test, we can conclude our model passes the "no perfect collinearity" assumption.

```{r Assessing Multicollinearity}
vif(model_complex)
```

[**Linear Conditional Expectation**]{.underline}

In order to properly apply a multiple regression analysis, we need to ensure linear relationships exist between our features and outcome variable. In case we observe non-linear relationships, we can apply various transformations (i.e., log transformation or polynomial terms) to ensure our data is linear enough to have a nicely fitted Best Linear Predictor (BLP). By leveraging the correlation matrix above, we can leverage the ocular test to observe the relationships between our features and outcome variable, $Growth.Overall.Reading.and.Math$. Because no non-linear relationships are present, we can proceed with our data as is, without implementing any transformations on the features. Additionally, the histogram below shows that our outcome variable appears to be normally distributed, further supporting the claim that no transformations need to be executed.

```{r}
hist(my_data$Growth.Overall.Reading.and.Math)
```

Since our data is IID, has no perfect collinearity, and is linear in nature, we pass all of our large linear model assumptions and can therefore proceed with the use of a multiple regression model.

# Key Modeling Decisions

# Regression Table

# Discussion of Results

# Discussion of Limitations (Statistical Limitations, Structural Limitations)

# Conclusion
