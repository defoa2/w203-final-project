---
title: "YouTube Video Views Relationship with Length and Rating"
output: 'pdf_document'  
classoption: landscape
fontsize: 12pt
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(wooldridge)
library(car)
library(lmtest)
library(sandwich)
library(gridExtra)
library(stargazer)
library(ggplot2)
library(GGally)
```

## 1.1 I.I.D.

Cheng, Dale, and Liu at Simon Fraser University used a breadth-first search to collect an abundance of YoutTube data. While "the crawl [on average] finds 73 thousand distinct videos each time in less than 9 hours", there still exists some concerns about independence. Given the nature of the breadth-first approach, the data may contain some clustering of similar videos, resulting in dependence between samples. In other words, since the scraped videos are on top lists, this would mean that these videos inform each other since they are already popular. In fact, the authors mention that the algorithm "checks the list of related videos and adds any new ones to the queue."

Moreover, the data was collected under the following categories: "Most Viewed", "Top Rated", "Most Discussed", for "Today", "This Week", "This Month" and "All Time". It could be the case that a specific trend is going viral, reinforcing the selection of similar videos amongst those collected by the crawler. Scraping only the most popular of videos is also a violation to random sampling, and may result in a skewed distribution that contains some bias towards popular videos. Said differently, the data may be non-normally distributed as the features do not adequately represent a wide array of genres, qualities, or other characteristics that represent the overall population of YouTube videos.

Additionally, because the crawler runs every 2-3 days, there may be duplicates in the data as the data is moved from the "Today" category to the "This Week" category, for example. These redundancies further violate the independence assumption as the video's features may be counted multiple times.

Given the way the data was collected, it is clear there are some violations to the IID assumption. The clustering of videos, skewed representation, duplicated data, and overrepresentation of popular videos are all factors that could impact our ability to pass the IID assumption, and therefore the use of a Classical Linear Model.

\newpage

## 1.2 Collinearity

The regression we are analysing is the following: $ln(views)=β_0 + β_1 rate + β_3 length$

Where the input, $rate$, is a representation of the average rating of the video and $length$ is the video duration in seconds.

The risk of collinearity here is $length$ and $rate$ could be related to one another.

```{r Create the model, message=FALSE, echo=FALSE}
youtube_data <- read.table(file = "videos.txt", sep = "\t", header = TRUE)
youtube_model = lm(log(views) ~ rate + length, data = youtube_data)
youtube_data <- subset(youtube_data, !is.na(views))
youtube_data$residuals <- resid(youtube_model)
```

The correlation between length and rate is not especially high and the scatterplot of the two explanatory variables does not show that they are highly correlated.

```{r correlation plot, echo=FALSE, message=FALSE}
ggpairs(videos[c(5, 7)]) + ggtitle("Correlation Plot")
```

The VIF for both length and rate is 1.03. VIF values below 5 are considered not highly correlation.

```{r vif values, echo=FALSE, message=FALSE}
vif(youtube_model)

vif_values <- vif(youtube_model)
barplot(vif_values, main = "VIF Values", horiz=TRUE, xlim = c(0, 6), col="steelblue")
abline(v = 5, lwd = 3, lty = 2)
```

From the correlation plot and the VIF values we can conclude that the explanatory variables are not highly correlated and the no perfect colinearity assumption is met.

\newpage

## 1.3. Linear Conditional Expectation

The regression we are analysing is the following: $ln(views)=β_0 + β_1 rate + β_3 length$

Linear Conditional Expectation (LCE) is a crucial requirement for regression analysis because we want to be sure that the combination of variables that we're using can be adequately represented by a linear function. It is worth disclaiming that the tests we use to identify LCE is nondispositive, meaning that we only have a marginal view based on each input, hence we cannot be certain if there truly is no unseen joint probability distribution.

The below show the relationship between the predicted values against the residuals. To further understand linear conditional expectation based on the input variables, both variables $Length$ and $Rate$ will be assessed.

```{r Create graph of residuals vs predicted, echo=FALSE, message=FALSE}
youtube_resid <- resid(youtube_model)
youtube_data$youtube_pred <- predict(youtube_model)

ggplot(youtube_data, aes(x = youtube_pred, y = residuals)) +
  geom_point() +                   # Add points to the plot
  stat_smooth() +                  # Smooth plot
  labs(x = "Predicted",            # X-axis label
       y = "Residuals",            # Y-axis label
       title = "Scatterplot of Predicted vs. Residuals")  # Plot title

```

In assessing the input $Rate$ against the model residuals which can seen on the left plot, there appears to be some noise, particularly as the ratings are between 2 to 5, the smoothing average shown as the blue line, appears to hover around the 0 mean line, however it is difficult to determine fully if this meets the LCE condition.

With the relationship between $Length$ and the model residuals, there is an issue with the fanning effect, where there is a concentration of data points towards shorter videos and significantly less longer videos. As a result, we see the smoothing average take on a nonlinear form that could imply another breach to the LCE condition.

```{r, echo=FALSE, message=FALSE}
# Create the scatterplot on rate
plot1 <- ggplot(youtube_data, aes(x = rate, y = residuals)) +
  geom_point() +                   # Add points to the plot
  stat_smooth() +                  # Smooth plot
  labs(x = "Rate",                 # X-axis label
       y = "Residuals",            # Y-axis label
       title = "Scatterplot of Residuals vs. Rate")  # Plot title
```

```{r, echo=FALSE, message=FALSE}
# Create the scatterplot on rate
plot2 <- ggplot(youtube_data, aes(x = length, y = residuals)) +
  geom_point() +                    # Add points to the plot
  stat_smooth() +                   # Smooth plot
  labs(x = "Length",                # X-axis label
       y = "Views",                 # Y-axis label
       title = "Scatterplot of Residuals vs. Length")  # Plot title
```

```{r}
grid.arrange(plot1, plot2, ncol = 2)
```

From these plots, they both imply potential breaches to LCE, however it's difficult to be certain with $Rate$ due to the noise, whereas with $Length$, the fanning effect also makes it difficult to be certain, but both are not clear 0 means, the inclination would be towards the model not meeting LCE requirements.

\newpage

## 1.4 Homoskedastic Errors

The homoskedastic errors assumption is that the residuals have equal variance for all predicted values of the model.

By examining the residual plot, we can see that there appears to be 3 distint clusters that are formed in the residual plot that have differing variances, with the middle cluster having the largest variance. Additionally, in the middle cluster, there seems to be a fanning out of the residuals as the fitted values increases.

```{r Residual plot, echo=FALSE, message=FALSE}
p <- ggplot(youtube_model, aes(x= .fitted,
                y = .resid)) +
  geom_point() + 
  labs(title = "Fitted vs Residual Plot",
       x = "Fitted Values",
       y = "Residuals")

p
```

This assumption can also be assessed by running a Breusch-Pagan test. The null assumption of the Breusch-Pagan test is that the errors are homoskedastic, the p-value is below 0.05, so we reject the null hypothesis and conclude that there is evidence of heteroskedasticity.

```{r, echo=FALSE, message=FALSE}
bptest(mod)
```

Based upon analyzing the residual plot and the Breusch-Pagan test, the homoskedastic error assumption is not met.

\newpage

## 1.5 Normal Distribution of Errors

The last assumption of the Classical Linear Model is that it must have normally distributed errors. As we can see in the QQ-plot below, the vast majority of the points fall on the reference line, indicating that the normally distributed errors assumption is met.

```{r}
qqnorm(residuals(log_model))
qqline(residuals(log_model))
```

Another way to test the normally distributed errors assumption is by plotting a histogram of the residuals. As we can see below, the residuals follow a nice bell shaped distribution, where the majority of residuals are good predictions, clustered around zero.

```{r}
hist(residuals(log_model), main='Histogram of Normally Distributed Residuals', xlab='Residuals', breaks=25)
```

Last but not least, we can run the Shaprio-Wilk test on the residuals of the fitted model. This test will tell us whether the residuals are normally distributed. A value close to 1 is indicative of a normal distribution, and in this case, the null hypothesis states that the data is normally distributed.

Because the Shapiro-Wilk test only works with 3-5000 sammples, we first need to shuffle our dataframe before using only half the data for the test.

```{r}
#run shapiro test with half the df because the full df has 9.4k rows and the shapiro test can only run with 3 - 5000 rows. 

set.seed(1)
youtube_data_shuffled <- youtube_data[sample(nrow(youtube_data)),]

#split the data in half
half_n <- nrow(youtube_data_shuffled) / 2
youtube_data_half <- youtube_data_shuffled[1:half_n,]

#fit the model on the half data
log_model_half <- lm(log_views ~ rate + length, data = youtube_data_half)

#do the Shapiro-Wilk test on the residuals of the model fitted on the half data
shapiro.test(residuals(log_model_half))
```

The results above are conflicting to say the least. On one hand, we have a high W score which indicates the residuals of the model is normally distributed. On the other hand, we have a significant p-value when alpha = 0.05. We therefore reject the null hypothesis, essentially stating the residuals are not normally distributed. This may be due to a having a large sample size, where even minor deviations from normality can result in a significant p-value.

Doing the same test above, but with only a quarter of the data, leads to consistent interpretation with both the W statistic and p-value stating the residuals are normally distributed.

```{r}
#run shapiro test with a quarter of the df to assess impact on a smaller sample

set.seed(1)
youtube_data_shuffled <- youtube_data[sample(nrow(youtube_data)),]

#split the data in half
half_n <- nrow(youtube_data_shuffled) / 4
youtube_data_half <- youtube_data_shuffled[1:half_n,]

#fit the model on the half data
log_model_half <- lm(log_views ~ rate + length, data = youtube_data_half)

#do the Shapiro-Wilk test on the residuals of the model fitted on the half data
shapiro.test(residuals(log_model_half))
```

The point above illustrates the importance of doing a multitude of tests, including ocular tests that examine the visual appearance of QQ plots and histograms alike. Overall, from the results of the ocular test and after altering the sample that serves as input for the Shaprio-Wilk normality test, we can safely conclude that our linear model passes the CLM assumption of normally distributed residuals.
